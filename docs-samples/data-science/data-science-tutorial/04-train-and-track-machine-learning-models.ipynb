{
    "cells": [{
            "cell_type": "markdown",
            "source": ["# Module 4: Train and register a machine learning model\r\n", "In this module you will learn to train a machine learning model to predict the total ride duration (tripDuration) of yellow taxi trips in New York City based on various factors such as pickup and drop-off locations, distance, date, time, number of passengers, and rate code.\r\n", "\r\n", "Once a model is trained, you will learn to register the trained model, and log hyperaparameters used and evaluation metrics using Trident's native integration with the MLflow framework.\r\n", "\r\n", "[MLflow](https://mlflow.org/docs/latest/index.html) is an open source platform for managing the machine learning lifecycle with features like Tracking, Models, and Model Registry. MLflow is natively integrated with Trident Data Science Experience."],
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["Please add the lakehouse you created earlier as the default lakehouse in this notebook."],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        }, {
            "cell_type": "markdown",
            "source": ["#### Import mlflow and create an experiment to log the run"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["# Create Experiment to Track and register model with mlflow\r\n", "import mlflow\r\n", "print(f\"mlflow lbrary version: {mlflow.__version__}\")\r\n", "EXPERIMENT_NAME = \"nyctaxi_tripduration\"\r\n", "mlflow.set_experiment(EXPERIMENT_NAME)"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 3,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3635782Z",
                            "session_start_time": "2023-05-08T06:54:40.9503133Z",
                            "execution_start_time": "2023-05-08T06:54:50.409838Z",
                            "execution_finish_time": "2023-05-08T06:55:13.7919158Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 0,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "4a87035d-eb66-4a9e-ac6a-e996598bf69d"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 3, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["mlflow lbrary version: 2.1.1\n"]
                }, {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": ["2023/05/08 06:54:53 INFO mlflow.tracking.fluent: Experiment with name 'nyctaxi_tripduration' does not exist. Creating a new experiment.\n"]
                }, {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "<Experiment: artifact_location='', creation_time=1683528903111, experiment_id='803a41bb-9147-4782-bb5b-ccbc239b6beb', last_update_time=None, lifecycle_stage='active', name='nyctaxi_tripduration', tags={}>"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 1,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Read Cleansed data from lakehouse delta table (saved in module 3)"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["SEED = 1234\r\n", "# note: From the perspective of the tutorial, we are sampling training data to speed up the execution.\r\n", "training_df = spark.read.format(\"delta\").load(\"Tables/nyctaxi_prep\").sample(fraction = 0.5, seed = SEED)"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 4,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3679617Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:55:14.9123974Z",
                            "execution_finish_time": "2023-05-08T06:55:20.2966986Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 3,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 0,
                                        "dataRead": 4880,
                                        "rowCount": 50,
                                        "jobId": 9,
                                        "name": "toString at String.java:2994",
                                        "description": "Delta: Job group for statement 4:\nSEED = 1234\n# note: From the perspective of the tutorial, we are sampling training data to speed up the execution.\ntraining_df = spark.read.format(\"delta\").load(\"Tables/nyctaxi_prep\").sample(fraction = 0.5, seed = SEED): Compute snapshot for version: 0",
                                        "submissionTime": "2023-05-08T06:55:19.615GMT",
                                        "completionTime": "2023-05-08T06:55:19.661GMT",
                                        "stageIds": [15, 13, 14],
                                        "jobGroup": "4",
                                        "status": "SUCCEEDED",
                                        "numTasks": 52,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 51,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 2,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 4880,
                                        "dataRead": 18035,
                                        "rowCount": 66,
                                        "jobId": 8,
                                        "name": "toString at String.java:2994",
                                        "description": "Delta: Job group for statement 4:\nSEED = 1234\n# note: From the perspective of the tutorial, we are sampling training data to speed up the execution.\ntraining_df = spark.read.format(\"delta\").load(\"Tables/nyctaxi_prep\").sample(fraction = 0.5, seed = SEED): Compute snapshot for version: 0",
                                        "submissionTime": "2023-05-08T06:55:18.869GMT",
                                        "completionTime": "2023-05-08T06:55:19.583GMT",
                                        "stageIds": [12, 11],
                                        "jobGroup": "4",
                                        "status": "SUCCEEDED",
                                        "numTasks": 51,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 50,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 18035,
                                        "dataRead": 31221,
                                        "rowCount": 32,
                                        "jobId": 7,
                                        "name": "toString at String.java:2994",
                                        "description": "Delta: Job group for statement 4:\nSEED = 1234\n# note: From the perspective of the tutorial, we are sampling training data to speed up the execution.\ntraining_df = spark.read.format(\"delta\").load(\"Tables/nyctaxi_prep\").sample(fraction = 0.5, seed = SEED): Compute snapshot for version: 0",
                                        "submissionTime": "2023-05-08T06:55:15.869GMT",
                                        "completionTime": "2023-05-08T06:55:16.506GMT",
                                        "stageIds": [10],
                                        "jobGroup": "4",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "dd763256-a2b1-49b7-bc58-ca2a125947da"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 4, Finished, Available)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 2,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Perform random split to get train and test datasets and define categorical and numeric features"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["TRAIN_TEST_SPLIT = [0.75, 0.25]\r\n", "train_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\r\n", "\r\n", "# Cache the dataframes to improve the speed of repeatable reads\r\n", "train_df.cache()\r\n", "test_df.cache()\r\n", "\r\n", "print(f\"train set count:{train_df.count()}\")\r\n", "print(f\"test set count:{test_df.count()}\")\r\n", "\r\n", "categorical_features = [\"storeAndFwdFlag\",\"timeBins\",\"vendorID\",\"weekDayName\",\"pickupHour\",\"rateCodeId\",\"paymentType\"]\r\n", "numeric_features = ['passengerCount', \"tripDistance\"]"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 5,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.368888Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:55:21.4205521Z",
                            "execution_finish_time": "2023-05-08T06:56:24.1891658Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 6,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 0,
                                        "dataRead": 767,
                                        "rowCount": 13,
                                        "jobId": 15,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 5:\nTRAIN_TEST_SPLIT = [0.75, 0.25]\ntrain_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\n\n# Cache the dataframes to improve the speed of repeatable reads\ntrain_df.cache()\ntest_df.cache()\n\nprint(f\"train set count:{train_df.count()}\")\nprint(f\"test set count:{test_df.count()}\")\n\ncategorical_features = [\"storeAndFwdFlag\",\"timeBins\",\"vendorID\",\"weekDayName\",\"pickupHour\",\"rateCodeId\",\"paymentType\"]\nnumeric_features = ['passengerCount', \"tripDistance\"]",
                                        "submissionTime": "2023-05-08T06:56:23.086GMT",
                                        "completionTime": "2023-05-08T06:56:23.103GMT",
                                        "stageIds": [24, 25],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 14,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 13,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 767,
                                        "dataRead": 1282332012,
                                        "rowCount": 46031000,
                                        "jobId": 14,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 5:\nTRAIN_TEST_SPLIT = [0.75, 0.25]\ntrain_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\n\n# Cache the dataframes to improve the speed of repeatable reads\ntrain_df.cache()\ntest_df.cache()\n\nprint(f\"train set count:{train_df.count()}\")\nprint(f\"test set count:{test_df.count()}\")\n\ncategorical_features = [\"storeAndFwdFlag\",\"timeBins\",\"vendorID\",\"weekDayName\",\"pickupHour\",\"rateCodeId\",\"paymentType\"]\nnumeric_features = ['passengerCount', \"tripDistance\"]",
                                        "submissionTime": "2023-05-08T06:55:58.846GMT",
                                        "completionTime": "2023-05-08T06:56:23.069GMT",
                                        "stageIds": [23],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 13,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 13,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 13,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 767,
                                        "rowCount": 13,
                                        "jobId": 13,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 5:\nTRAIN_TEST_SPLIT = [0.75, 0.25]\ntrain_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\n\n# Cache the dataframes to improve the speed of repeatable reads\ntrain_df.cache()\ntest_df.cache()\n\nprint(f\"train set count:{train_df.count()}\")\nprint(f\"test set count:{test_df.count()}\")\n\ncategorical_features = [\"storeAndFwdFlag\",\"timeBins\",\"vendorID\",\"weekDayName\",\"pickupHour\",\"rateCodeId\",\"paymentType\"]\nnumeric_features = ['passengerCount', \"tripDistance\"]",
                                        "submissionTime": "2023-05-08T06:55:58.662GMT",
                                        "completionTime": "2023-05-08T06:55:58.700GMT",
                                        "stageIds": [21, 22],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 14,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 13,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 767,
                                        "dataRead": 1282426207,
                                        "rowCount": 46031000,
                                        "jobId": 12,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 5:\nTRAIN_TEST_SPLIT = [0.75, 0.25]\ntrain_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\n\n# Cache the dataframes to improve the speed of repeatable reads\ntrain_df.cache()\ntest_df.cache()\n\nprint(f\"train set count:{train_df.count()}\")\nprint(f\"test set count:{test_df.count()}\")\n\ncategorical_features = [\"storeAndFwdFlag\",\"timeBins\",\"vendorID\",\"weekDayName\",\"pickupHour\",\"rateCodeId\",\"paymentType\"]\nnumeric_features = ['passengerCount', \"tripDistance\"]",
                                        "submissionTime": "2023-05-08T06:55:22.354GMT",
                                        "completionTime": "2023-05-08T06:55:58.642GMT",
                                        "stageIds": [20],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 13,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 13,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 13,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 16086,
                                        "rowCount": 15,
                                        "jobId": 11,
                                        "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
                                        "description": "Delta: Job group for statement 5:\nTRAIN_TEST_SPLIT = [0.75, 0.25]\ntrain_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\n\n# Cache the dataframes to improve the speed of repeatable reads\ntrain_df.cache()\ntest_df.cache()\n\nprint(f\"train set count:{train_df.count()}\")\nprint(f\"test set count:{test_df.count()}\")\n\ncategorical_features = [\"storeAndFwdFlag\",\"timeBins\",\"vendorID\",\"weekDayName\",\"pickupHour\",\"rateCodeId\",\"paymentType\"]\nnumeric_features = ['passengerCount', \"tripDistance\"]: Filtering files for query",
                                        "submissionTime": "2023-05-08T06:55:21.992GMT",
                                        "completionTime": "2023-05-08T06:55:22.150GMT",
                                        "stageIds": [19, 18],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 51,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 50,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 16086,
                                        "rowCount": 15,
                                        "jobId": 10,
                                        "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
                                        "description": "Delta: Job group for statement 5:\nTRAIN_TEST_SPLIT = [0.75, 0.25]\ntrain_df, test_df = training_df.randomSplit(TRAIN_TEST_SPLIT, seed=SEED)\n\n# Cache the dataframes to improve the speed of repeatable reads\ntrain_df.cache()\ntest_df.cache()\n\nprint(f\"train set count:{train_df.count()}\")\nprint(f\"test set count:{test_df.count()}\")\n\ncategorical_features = [\"storeAndFwdFlag\",\"timeBins\",\"vendorID\",\"weekDayName\",\"pickupHour\",\"rateCodeId\",\"paymentType\"]\nnumeric_features = ['passengerCount', \"tripDistance\"]: Filtering files for query",
                                        "submissionTime": "2023-05-08T06:55:21.497GMT",
                                        "completionTime": "2023-05-08T06:55:21.725GMT",
                                        "stageIds": [16, 17],
                                        "jobGroup": "5",
                                        "status": "SUCCEEDED",
                                        "numTasks": 51,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 50,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "77df141c-597f-4288-a320-a29dd0492f77"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 5, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["train set count:17256796\ntest set count:5754996\n"]
                }
            ],
            "execution_count": 3,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Define the steps to perform additional feature engineering and train the model using Spark ML pipelines and Microsoft SynapseML library\r\n", "You can learn more about Spark ML pipelines [here](https://spark.apache.org/docs/latest/ml-pipeline.html), and SynapseML is documented [here](https://microsoft.github.io/SynapseML/docs/about/)\r\n", "\r\n", "The algorithm used for this tutorial, [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/) is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms. It is an open source project developed by Microsoft and supports regression, classification and many other machine learning scenarios. Its main advantages are faster training speed, lower memory usage, better accuracy, and support for distributed learning."],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\r\n", "from pyspark.ml import Pipeline\r\n", "from synapse.ml.core.platform import *\r\n", "from synapse.ml.lightgbm import LightGBMRegressor\r\n", "\r\n", "# Define a pipeline steps for training a LightGBMRegressor regressor model\r\n", "def lgbm_pipeline(categorical_features,numeric_features, hyperparameters):\r\n", "    # String indexer\r\n", "    stri = StringIndexer(inputCols=categorical_features, \r\n", "                        outputCols=[f\"{feat}Idx\" for feat in categorical_features]).setHandleInvalid(\"keep\")\r\n", "    # encode categorical/indexed columns\r\n", "    ohe = OneHotEncoder(inputCols= stri.getOutputCols(),  \r\n", "                        outputCols=[f\"{feat}Enc\" for feat in categorical_features])\r\n", "    \r\n", "    # convert all feature columns into a vector\r\n", "    featurizer = VectorAssembler(inputCols=ohe.getOutputCols() + numeric_features, outputCol=\"features\")\r\n", "\r\n", "    # Define the LightGBM regressor\r\n", "    lgr = LightGBMRegressor(\r\n", "        objective = hyperparameters[\"objective\"],\r\n", "        alpha = hyperparameters[\"alpha\"],\r\n", "        learningRate = hyperparameters[\"learning_rate\"],\r\n", "        numLeaves = hyperparameters[\"num_leaves\"],\r\n", "        labelCol=\"tripDuration\",\r\n", "        numIterations = hyperparameters[\"iterations\"],\r\n", "    )\r\n", "    # Define the steps and sequence of the SPark ML pipeline\r\n", "    ml_pipeline = Pipeline(stages=[stri, ohe, featurizer, lgr])\r\n", "    return ml_pipeline\r\n"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 6,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3758597Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:56:25.2625578Z",
                            "execution_finish_time": "2023-05-08T06:56:25.6028613Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 0,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "77d42f20-a644-4a19-8eda-aad67facb410"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 6, Finished, Available)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 4,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Define Training Hyperparameters\r\n", "Hyperparameters are the parameters that you can change to control how a machine learning model is trained. Hyperparameters can affect the speed, quality and accuracy of the model. Some common methods to find the best hyperparameters are by testing different values, using a grid or random search, or using a more advanced optimization technique.\r\n", "The hyperparameters for the lightgbm model in this tutorial have been pre-tuned using a distributed gridsearch run using [hyperopt](https://github.com/hyperopt/hyperopt)"],
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Model Run 1: Using default lightgbm hyperparameters"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["# Default hyperparameters for LightGBM Model\r\n", "LGBM_PARAMS = {\"objective\":\"regression\",\r\n", "    \"alpha\":0.9,\r\n", "    \"learning_rate\":0.1,\r\n", "    \"num_leaves\":31,\r\n", "    \"iterations\":100}"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 7,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3769966Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:56:26.7476777Z",
                            "execution_finish_time": "2023-05-08T06:56:27.103696Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 0,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "b311354f-d27a-414f-bb32-8b775647d2a7"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 7, Finished, Available)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 5,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Fit the defined pipeline on the training dataframe and generate predictions on the test dataset"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["if mlflow.active_run() is None:\r\n", "    mlflow.start_run()\r\n", "run = mlflow.active_run()\r\n", "print(f\"Active experiment run_id: {run.info.run_id}\")\r\n", "lg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\r\n", "lg_model = lg_pipeline.fit(train_df)\r\n", "\r\n", "# Get Predictions\r\n", "lg_predictions = lg_model.transform(test_df)\r\n", "## Caching predictions to run model evaluation faster\r\n", "lg_predictions.cache()\r\n", "print(f\"Prediction run for {lg_predictions.count()} samples\")"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 8,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3849016Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:56:28.2995355Z",
                            "execution_finish_time": "2023-05-08T06:58:52.2672455Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 6,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 0,
                                        "dataRead": 767,
                                        "rowCount": 13,
                                        "jobId": 21,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 8:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\nlg_model = lg_pipeline.fit(train_df)\n\n# Get Predictions\nlg_predictions = lg_model.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions.cache()\nprint(f\"Prediction run for {lg_predictions.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:58:49.952GMT",
                                        "completionTime": "2023-05-08T06:58:49.972GMT",
                                        "stageIds": [33, 32],
                                        "jobGroup": "8",
                                        "status": "SUCCEEDED",
                                        "numTasks": 14,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 13,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 767,
                                        "dataRead": 714202176,
                                        "rowCount": 595,
                                        "jobId": 20,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 8:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\nlg_model = lg_pipeline.fit(train_df)\n\n# Get Predictions\nlg_predictions = lg_model.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions.cache()\nprint(f\"Prediction run for {lg_predictions.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:57:31.849GMT",
                                        "completionTime": "2023-05-08T06:58:49.934GMT",
                                        "stageIds": [31],
                                        "jobGroup": "8",
                                        "status": "SUCCEEDED",
                                        "numTasks": 13,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 13,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 13,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 2140848048,
                                        "rowCount": 1734,
                                        "jobId": 19,
                                        "name": "collect at LightGBMBase.scala:597",
                                        "description": "Job group for statement 8:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\nlg_model = lg_pipeline.fit(train_df)\n\n# Get Predictions\nlg_predictions = lg_model.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions.cache()\nprint(f\"Prediction run for {lg_predictions.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:56:37.189GMT",
                                        "completionTime": "2023-05-08T06:57:31.443GMT",
                                        "stageIds": [30],
                                        "jobGroup": "8",
                                        "status": "SUCCEEDED",
                                        "numTasks": 8,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 8,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 8,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 207605448,
                                        "rowCount": 1,
                                        "jobId": 18,
                                        "name": "first at LightGBMBase.scala:470",
                                        "description": "Job group for statement 8:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\nlg_model = lg_pipeline.fit(train_df)\n\n# Get Predictions\nlg_predictions = lg_model.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions.cache()\nprint(f\"Prediction run for {lg_predictions.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:56:36.839GMT",
                                        "completionTime": "2023-05-08T06:56:37.029GMT",
                                        "stageIds": [29],
                                        "jobGroup": "8",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 13296,
                                        "rowCount": 13,
                                        "jobId": 17,
                                        "name": "collect at StringIndexer.scala:204",
                                        "description": "Job group for statement 8:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\nlg_model = lg_pipeline.fit(train_df)\n\n# Get Predictions\nlg_predictions = lg_model.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions.cache()\nprint(f\"Prediction run for {lg_predictions.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:56:36.092GMT",
                                        "completionTime": "2023-05-08T06:56:36.151GMT",
                                        "stageIds": [27, 28],
                                        "jobGroup": "8",
                                        "status": "SUCCEEDED",
                                        "numTasks": 14,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 13,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 13296,
                                        "dataRead": 2140848048,
                                        "rowCount": 1747,
                                        "jobId": 16,
                                        "name": "collect at StringIndexer.scala:204",
                                        "description": "Job group for statement 8:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline = lgbm_pipeline(categorical_features,numeric_features,LGBM_PARAMS)\nlg_model = lg_pipeline.fit(train_df)\n\n# Get Predictions\nlg_predictions = lg_model.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions.cache()\nprint(f\"Prediction run for {lg_predictions.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:56:29.670GMT",
                                        "completionTime": "2023-05-08T06:56:36.081GMT",
                                        "stageIds": [26],
                                        "jobGroup": "8",
                                        "status": "SUCCEEDED",
                                        "numTasks": 13,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 13,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 13,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "e37ecd6a-d322-4538-a8bc-4faaf0ab0670"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 8, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["Active experiment run_id: 47e80a47-a78f-45bb-925d-a8450e56ace3\nPrediction run for 5754996 samples\n"]
                }
            ],
            "execution_count": 6,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Compute Model Statistics for evaluating performance of the trained LightGBMRegressor model"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["from synapse.ml.train import ComputeModelStatistics\r\n", "lg_metrics = ComputeModelStatistics(\r\n", "    evaluationMetric=\"regression\", labelCol=\"tripDuration\", scoresCol=\"prediction\"\r\n", ").transform(lg_predictions) \r\n", "display(lg_metrics)"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 23,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T07:09:49.5327719Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T07:09:50.4354862Z",
                            "execution_finish_time": "2023-05-08T07:09:51.4910707Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 1,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 4576,
                                        "dataRead": 92359840,
                                        "rowCount": 608,
                                        "jobId": 51,
                                        "name": "treeAggregate at Statistics.scala:58",
                                        "description": "Job group for statement 23:\nfrom synapse.ml.train import ComputeModelStatistics\nlg_metrics = ComputeModelStatistics(\n    evaluationMetric=\"regression\", labelCol=\"tripDuration\", scoresCol=\"prediction\"\n).transform(lg_predictions) \ndisplay(lg_metrics)",
                                        "submissionTime": "2023-05-08T07:09:50.320GMT",
                                        "completionTime": "2023-05-08T07:09:51.181GMT",
                                        "stageIds": [72, 73],
                                        "jobGroup": "23",
                                        "status": "SUCCEEDED",
                                        "numTasks": 16,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 16,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 16,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 2,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "42698b45-10b3-4db2-8067-71a8305f4cf4"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 23, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.synapse.widget-view+json": {
                            "widget_id": "838a4109-46a5-43e7-92c9-e60eb76d273f",
                            "widget_type": "Synapse.DataFrame"
                        },
                        "text/plain": "SynapseWidget(Synapse.DataFrame, 838a4109-46a5-43e7-92c9-e60eb76d273f)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 21,
            "metadata": {
                "collapsed": false
            }
        }, {
            "cell_type": "markdown",
            "source": ["#### Register the trained LightGBMRegressor model using MLflow"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["from mlflow.models.signature import ModelSignature \r\n", "from mlflow.types.utils import _infer_schema \r\n", "\r\n", "# Define a function to register a spark model\r\n", "def register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\r\n", "        # log the model, parameters and metrics\r\n", "        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \r\n", "        mlflow.log_params(hyperparameters) \r\n", "        mlflow.log_metrics(metrics) \r\n", "        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \r\n", "        print(f\"Model saved in run{run.info.run_id}\") \r\n", "        print(f\"Model URI: {model_uri}\")\r\n", "        return model_uri\r\n", "\r\n", "# Define Signature object \r\n", "sig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \r\n", "                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \r\n", "\r\n", "ALGORITHM = \"lightgbm\" \r\n", "model_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\r\n", "\r\n", "# Create a 'dict' object that contains values of metrics\r\n", "lg_metrics_dict = json.loads(lg_metrics.toJSON().first())\r\n", "\r\n", "# Call model register function\r\n", "model_uri = register_spark_model(run = run,\r\n", "                                model = lg_model, \r\n", "                                model_name = model_name, \r\n", "                                signature = sig, \r\n", "                                metrics = lg_metrics_dict, \r\n", "                                hyperparameters = LGBM_PARAMS)\r\n", "mlflow.end_run()"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 10,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3872029Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:58:57.198914Z",
                            "execution_finish_time": "2023-05-08T06:59:37.6686934Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 9,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 665,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 32,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:07.571GMT",
                                        "completionTime": "2023-05-08T06:59:08.233GMT",
                                        "stageIds": [47],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 447,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 31,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:06.114GMT",
                                        "completionTime": "2023-05-08T06:59:06.794GMT",
                                        "stageIds": [46],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 814,
                                        "dataRead": 90,
                                        "rowCount": 2,
                                        "jobId": 30,
                                        "name": "parquet at OneHotEncoder.scala:407",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:04.666GMT",
                                        "completionTime": "2023-05-08T06:59:05.380GMT",
                                        "stageIds": [45, 44],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 2,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 90,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 29,
                                        "name": "parquet at OneHotEncoder.scala:407",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:04.622GMT",
                                        "completionTime": "2023-05-08T06:59:04.638GMT",
                                        "stageIds": [43],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 537,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 28,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:03.038GMT",
                                        "completionTime": "2023-05-08T06:59:03.873GMT",
                                        "stageIds": [42],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 1169,
                                        "dataRead": 644,
                                        "rowCount": 2,
                                        "jobId": 27,
                                        "name": "parquet at StringIndexer.scala:499",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:01.389GMT",
                                        "completionTime": "2023-05-08T06:59:02.205GMT",
                                        "stageIds": [40, 41],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 2,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 644,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 26,
                                        "name": "parquet at StringIndexer.scala:499",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:01.346GMT",
                                        "completionTime": "2023-05-08T06:59:01.362GMT",
                                        "stageIds": [39],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 557,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 25,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:59:00.035GMT",
                                        "completionTime": "2023-05-08T06:59:00.672GMT",
                                        "stageIds": [38],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 313,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 24,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 10:\nfrom mlflow.models.signature import ModelSignature \nfrom mlflow.types.utils import _infer_schema \n\n# Define a function to register a spark model\ndef register_spark_model(run, model, model_name,signature,metrics, hyperparameters):\n        # log the model, parameters and metrics\n        mlflow.spark.log_model(model, artifact_path = model_name, signature=signature, registered_model_name = model_name, dfs_tmpdir=\"Files/tmp/mlflow\") \n        mlflow.log_params(hyperparameters) \n        mlflow.log_metrics(metrics) \n        model_uri = f\"runs:/{run.info.run_id}/{model_name}\" \n        print(f\"Model saved in run{run.info.run_id}\") \n        print(f\"Model URI: {model_uri}\")\n        return model_uri\n\n# Define Signature object \nsig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\"))) \n\nALGORITHM = \"lightgbm\" \nmodel_name = f\"{EXPERIMENT_NAME}_{ALGORITHM}\"\n\n# Call model register function\nmodel_uri = regis...",
                                        "submissionTime": "2023-05-08T06:58:58.411GMT",
                                        "completionTime": "2023-05-08T06:58:59.257GMT",
                                        "stageIds": [37],
                                        "jobGroup": "10",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "f44e1c6a-4ba9-4ac5-b39a-a6bc7cc349dd"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 10, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": ["/tmp/ipykernel_7705/2352378258.py:16: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  sig = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)),\n2023/05/08 06:59:19 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp_ldwrdfe/model, flavor: spark), fall back to return ['pyspark==3.3.1']. Set logging level to DEBUG to see the full traceback.\n/home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nSuccessfully registered model 'nyctaxi_tripduration_lightgbm'.\n2023/05/08 06:59:33 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: nyctaxi_tripduration_lightgbm, version 1\nCreated version '1' of model 'nyctaxi_tripduration_lightgbm'.\n"]
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["Model saved in run47e80a47-a78f-45bb-925d-a8450e56ace3\nModel URI: runs:/47e80a47-a78f-45bb-925d-a8450e56ace3/nyctaxi_tripduration_lightgbm\n"]
                }
            ],
            "execution_count": 8,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Model Run 2: Using tuned lightgbm hyperparameters and remove paymentType \r\n", "Since paymentType is usually selected at the end of a trip, we hypothize that it shouldn't be useful to predict trip duration."],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["# Tuned hyperparameters for LightGBM Model\r\n", "TUNED_LGBM_PARAMS = {\"objective\":\"regression\",\r\n", "    \"alpha\":0.08373361416254149,\r\n", "    \"learning_rate\":0.0801709918703746,\r\n", "    \"num_leaves\":92,\r\n", "    \"iterations\":200}\r\n", "\r\n", "# Remove paymentType\r\n", "categorical_features.remove(\"paymentType\")"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 11,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3937554Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:59:38.8940543Z",
                            "execution_finish_time": "2023-05-08T06:59:39.2579897Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 0,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "2a29e256-31e3-4a8c-9901-15028f674568"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 11, Finished, Available)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 9,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["#### Fit the lightgbm pipeline with tuned hyperparameter on the training dataframe and generate predictions on the test dataset"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["if mlflow.active_run() is None:\r\n", "    mlflow.start_run()\r\n", "run = mlflow.active_run()\r\n", "print(f\"Active experiment run_id: {run.info.run_id}\")\r\n", "lg_pipeline_tn = lgbm_pipeline(categorical_features,numeric_features,TUNED_LGBM_PARAMS)\r\n", "lg_model_tn = lg_pipeline_tn.fit(train_df)\r\n", "\r\n", "# Get Predictions\r\n", "lg_predictions_tn = lg_model_tn.transform(test_df)\r\n", "## Caching predictions to run model evaluation faster\r\n", "lg_predictions_tn.cache()\r\n", "print(f\"Prediction run for {lg_predictions_tn.count()} samples\")"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 12,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3949102Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T06:59:40.3005994Z",
                            "execution_finish_time": "2023-05-08T07:03:16.6522547Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 6,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 0,
                                        "dataRead": 767,
                                        "rowCount": 13,
                                        "jobId": 38,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 12:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline_tn = lgbm_pipeline(categorical_features,numeric_features,TUNED_LGBM_PARAMS)\nlg_model_tn = lg_pipeline_tn.fit(train_df)\n\n# Get Predictions\nlg_predictions_tn = lg_model_tn.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions_tn.cache()\nprint(f\"Prediction run for {lg_predictions_tn.count()} samples\")",
                                        "submissionTime": "2023-05-08T07:03:14.230GMT",
                                        "completionTime": "2023-05-08T07:03:14.518GMT",
                                        "stageIds": [54, 55],
                                        "jobGroup": "12",
                                        "status": "SUCCEEDED",
                                        "numTasks": 14,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 13,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 767,
                                        "dataRead": 1282426207,
                                        "rowCount": 46031000,
                                        "jobId": 37,
                                        "name": "count at NativeMethodAccessorImpl.java:0",
                                        "description": "Job group for statement 12:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline_tn = lgbm_pipeline(categorical_features,numeric_features,TUNED_LGBM_PARAMS)\nlg_model_tn = lg_pipeline_tn.fit(train_df)\n\n# Get Predictions\nlg_predictions_tn = lg_model_tn.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions_tn.cache()\nprint(f\"Prediction run for {lg_predictions_tn.count()} samples\")",
                                        "submissionTime": "2023-05-08T07:01:10.493GMT",
                                        "completionTime": "2023-05-08T07:03:14.212GMT",
                                        "stageIds": [53],
                                        "jobGroup": "12",
                                        "status": "SUCCEEDED",
                                        "numTasks": 13,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 13,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 13,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 2140848048,
                                        "rowCount": 1734,
                                        "jobId": 36,
                                        "name": "collect at LightGBMBase.scala:597",
                                        "description": "Job group for statement 12:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline_tn = lgbm_pipeline(categorical_features,numeric_features,TUNED_LGBM_PARAMS)\nlg_model_tn = lg_pipeline_tn.fit(train_df)\n\n# Get Predictions\nlg_predictions_tn = lg_model_tn.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions_tn.cache()\nprint(f\"Prediction run for {lg_predictions_tn.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:59:46.633GMT",
                                        "completionTime": "2023-05-08T07:01:09.226GMT",
                                        "stageIds": [52],
                                        "jobGroup": "12",
                                        "status": "SUCCEEDED",
                                        "numTasks": 8,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 8,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 8,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 207605448,
                                        "rowCount": 1,
                                        "jobId": 35,
                                        "name": "first at LightGBMBase.scala:470",
                                        "description": "Job group for statement 12:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline_tn = lgbm_pipeline(categorical_features,numeric_features,TUNED_LGBM_PARAMS)\nlg_model_tn = lg_pipeline_tn.fit(train_df)\n\n# Get Predictions\nlg_predictions_tn = lg_model_tn.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions_tn.cache()\nprint(f\"Prediction run for {lg_predictions_tn.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:59:46.545GMT",
                                        "completionTime": "2023-05-08T06:59:46.587GMT",
                                        "stageIds": [51],
                                        "jobGroup": "12",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 0,
                                        "dataRead": 12513,
                                        "rowCount": 13,
                                        "jobId": 34,
                                        "name": "collect at StringIndexer.scala:204",
                                        "description": "Job group for statement 12:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline_tn = lgbm_pipeline(categorical_features,numeric_features,TUNED_LGBM_PARAMS)\nlg_model_tn = lg_pipeline_tn.fit(train_df)\n\n# Get Predictions\nlg_predictions_tn = lg_model_tn.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions_tn.cache()\nprint(f\"Prediction run for {lg_predictions_tn.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:59:46.049GMT",
                                        "completionTime": "2023-05-08T06:59:46.087GMT",
                                        "stageIds": [49, 50],
                                        "jobGroup": "12",
                                        "status": "SUCCEEDED",
                                        "numTasks": 14,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 13,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 12513,
                                        "dataRead": 2140848048,
                                        "rowCount": 1747,
                                        "jobId": 33,
                                        "name": "collect at StringIndexer.scala:204",
                                        "description": "Job group for statement 12:\nif mlflow.active_run() is None:\n    mlflow.start_run()\nrun = mlflow.active_run()\nprint(f\"Active experiment run_id: {run.info.run_id}\")\nlg_pipeline_tn = lgbm_pipeline(categorical_features,numeric_features,TUNED_LGBM_PARAMS)\nlg_model_tn = lg_pipeline_tn.fit(train_df)\n\n# Get Predictions\nlg_predictions_tn = lg_model_tn.transform(test_df)\n## Caching predictions to run model evaluation faster\nlg_predictions_tn.cache()\nprint(f\"Prediction run for {lg_predictions_tn.count()} samples\")",
                                        "submissionTime": "2023-05-08T06:59:40.636GMT",
                                        "completionTime": "2023-05-08T06:59:46.034GMT",
                                        "stageIds": [48],
                                        "jobGroup": "12",
                                        "status": "SUCCEEDED",
                                        "numTasks": 13,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 13,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 13,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "dc92aef8-3df2-48eb-aa26-f636a5f77295"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 12, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["Active experiment run_id: b1bb91e0-55cf-4302-86d5-53b8aba63d13\nPrediction run for 5754996 samples\n"]
                }
            ],
            "execution_count": 10,
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["lg_metrics_tn = ComputeModelStatistics(\r\n", "    evaluationMetric=\"regression\", labelCol=\"tripDuration\", scoresCol=\"prediction\"\r\n", ").transform(lg_predictions_tn)\r\n", "display(lg_metrics_tn)"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 13,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3961667Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T07:03:18.12572Z",
                            "execution_finish_time": "2023-05-08T07:03:23.4640486Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 2,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 0,
                                        "dataRead": 0,
                                        "rowCount": 0,
                                        "jobId": 40,
                                        "name": "runJob at PythonRDD.scala:172",
                                        "description": "Job group for statement 13:\nlg_metrics_tn = ComputeModelStatistics(\n    evaluationMetric=\"regression\", labelCol=\"tripDuration\", scoresCol=\"prediction\"\n).transform(lg_predictions_tn)\nlg_metricstn_dict = json.loads(lg_metrics_tn.toJSON().first())\ndisplay(lg_metrics_tn)",
                                        "submissionTime": "2023-05-08T07:03:19.822GMT",
                                        "completionTime": "2023-05-08T07:03:23.063GMT",
                                        "stageIds": [58],
                                        "jobGroup": "13",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 4576,
                                        "dataRead": 5210406048,
                                        "rowCount": 1372,
                                        "jobId": 39,
                                        "name": "treeAggregate at Statistics.scala:58",
                                        "description": "Job group for statement 13:\nlg_metrics_tn = ComputeModelStatistics(\n    evaluationMetric=\"regression\", labelCol=\"tripDuration\", scoresCol=\"prediction\"\n).transform(lg_predictions_tn)\nlg_metricstn_dict = json.loads(lg_metrics_tn.toJSON().first())\ndisplay(lg_metrics_tn)",
                                        "submissionTime": "2023-05-08T07:03:17.908GMT",
                                        "completionTime": "2023-05-08T07:03:19.775GMT",
                                        "stageIds": [56, 57],
                                        "jobGroup": "13",
                                        "status": "SUCCEEDED",
                                        "numTasks": 16,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 16,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 16,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 2,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "207dd647-2772-4b14-8ebd-93f875523aba"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 13, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.synapse.widget-view+json": {
                            "widget_id": "11961b27-2c93-4331-9f4c-a53e994a9907",
                            "widget_type": "Synapse.DataFrame"
                        },
                        "text/plain": "SynapseWidget(Synapse.DataFrame, 11961b27-2c93-4331-9f4c-a53e994a9907)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 11,
            "metadata": {
                "collapsed": false
            }
        }, {
            "cell_type": "markdown",
            "source": ["#### Register the trained LightGBMRegressor model using MLflow"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["# Define Signature object \r\n", "sig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \r\n", "                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\r\n", "\r\n", "# Create a 'dict' object that contains values of metrics\r\n", "lg_metricstn_dict = json.loads(lg_metrics_tn.toJSON().first())\r\n", "\r\n", "model_uri = register_spark_model(run = run,\r\n", "                                model = lg_model_tn, \r\n", "                                model_name = model_name, \r\n", "                                signature = sig_tn, \r\n", "                                metrics = lg_metricstn_dict, \r\n", "                                hyperparameters = TUNED_LGBM_PARAMS)\r\n", "mlflow.end_run()"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 14,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3974592Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T07:03:24.5869123Z",
                            "execution_finish_time": "2023-05-08T07:04:02.5791945Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 9,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [{
                                        "dataWritten": 665,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 49,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:36.273GMT",
                                        "completionTime": "2023-05-08T07:03:36.896GMT",
                                        "stageIds": [69],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 430,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 48,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:34.957GMT",
                                        "completionTime": "2023-05-08T07:03:35.611GMT",
                                        "stageIds": [68],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 814,
                                        "dataRead": 86,
                                        "rowCount": 2,
                                        "jobId": 47,
                                        "name": "parquet at OneHotEncoder.scala:407",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:33.456GMT",
                                        "completionTime": "2023-05-08T07:03:34.388GMT",
                                        "stageIds": [66, 67],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 2,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 86,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 46,
                                        "name": "parquet at OneHotEncoder.scala:407",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:33.419GMT",
                                        "completionTime": "2023-05-08T07:03:33.432GMT",
                                        "stageIds": [65],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 503,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 45,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:31.846GMT",
                                        "completionTime": "2023-05-08T07:03:32.656GMT",
                                        "stageIds": [64],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 1158,
                                        "dataRead": 603,
                                        "rowCount": 2,
                                        "jobId": 44,
                                        "name": "parquet at StringIndexer.scala:499",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:28.908GMT",
                                        "completionTime": "2023-05-08T07:03:31.179GMT",
                                        "stageIds": [63, 62],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 2,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 1,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 1,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 603,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 43,
                                        "name": "parquet at StringIndexer.scala:499",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:28.869GMT",
                                        "completionTime": "2023-05-08T07:03:28.883GMT",
                                        "stageIds": [61],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 526,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 42,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:27.509GMT",
                                        "completionTime": "2023-05-08T07:03:28.174GMT",
                                        "stageIds": [60],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }, {
                                        "dataWritten": 313,
                                        "dataRead": 0,
                                        "rowCount": 1,
                                        "jobId": 41,
                                        "name": "runJob at SparkHadoopWriter.scala:85",
                                        "description": "Job group for statement 14:\n# Define Signature object \nsig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)), \n                     outputs=_infer_schema(train_df.select(\"tripDuration\")))\nmodel_uri = register_spark_model(run = run,\n                                model = lg_model_tn, \n                                model_name = model_name, \n                                signature = sig_tn, \n                                metrics = lg_metricstn_dict, \n                                hyperparameters = TUNED_LGBM_PARAMS)\nmlflow.end_run()",
                                        "submissionTime": "2023-05-08T07:03:25.588GMT",
                                        "completionTime": "2023-05-08T07:03:26.796GMT",
                                        "stageIds": [59],
                                        "jobGroup": "14",
                                        "status": "SUCCEEDED",
                                        "numTasks": 1,
                                        "numActiveTasks": 0,
                                        "numCompletedTasks": 1,
                                        "numSkippedTasks": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numActiveStages": 0,
                                        "numCompletedStages": 1,
                                        "numSkippedStages": 0,
                                        "numFailedStages": 0,
                                        "killedTasksSummary": {}
                                    }
                                ],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "b5b0b88e-08bb-4b98-8241-3708f0dd1322"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 14, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": ["/tmp/ipykernel_7705/1750966625.py:2: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  sig_tn = ModelSignature(inputs=_infer_schema(train_df.select(categorical_features + numeric_features)),\n2023/05/08 07:03:45 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpu6fqzas2/model, flavor: spark), fall back to return ['pyspark==3.3.1']. Set logging level to DEBUG to see the full traceback.\nSuccessfully registered model 'nyctaxi_tripduration_lightgbm'.\n2023/05/08 07:03:59 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: nyctaxi_tripduration_lightgbm, version 2\nCreated version '2' of model 'nyctaxi_tripduration_lightgbm'.\n"]
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["Model saved in runb1bb91e0-55cf-4302-86d5-53b8aba63d13\nModel URI: runs:/b1bb91e0-55cf-4302-86d5-53b8aba63d13/nyctaxi_tripduration_lightgbm\n"]
                }
            ],
            "execution_count": 12,
            "metadata": {}
        }, {
            "cell_type": "markdown",
            "source": ["Note: if you do not see your model artifact in the workspace, please make sure to refresh your browser."],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        }, {
            "cell_type": "markdown",
            "source": ["#### You will need the below run_uri to execute the next tutorial"],
            "metadata": {}
        }, {
            "cell_type": "code",
            "source": ["print(f\"Please copy this run_uri: {model_uri}\")"],
            "outputs": [{
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": "07c294f4-0823-4082-8017-42be47f9a7ac",
                            "statement_id": 15,
                            "state": "finished",
                            "livy_statement_state": "available",
                            "queued_time": "2023-05-08T06:54:40.3986366Z",
                            "session_start_time": null,
                            "execution_start_time": "2023-05-08T07:04:03.6840866Z",
                            "execution_finish_time": "2023-05-08T07:04:04.0265972Z",
                            "spark_jobs": {
                                "numbers": {
                                    "SUCCEEDED": 0,
                                    "UNKNOWN": 0,
                                    "RUNNING": 0,
                                    "FAILED": 0
                                },
                                "jobs": [],
                                "limit": 20,
                                "rule": "ALL_DESC"
                            },
                            "parent_msg_id": "27aaa131-cb6e-4068-a66c-28d64bc91b44"
                        },
                        "text/plain": "StatementMeta(, 07c294f4-0823-4082-8017-42be47f9a7ac, 15, Finished, Available)"
                    },
                    "metadata": {}
                }, {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": ["Please copy this run_uri: runs:/b1bb91e0-55cf-4302-86d5-53b8aba63d13/nyctaxi_tripduration_lightgbm\n"]
                }
            ],
            "execution_count": 13,
            "metadata": {}
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "widgets": {},
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "enableDebugMode": false,
                "conf": {}
            }
        },
        "notebook_environment": {},
        "synapse_widget": {
            "version": "0.1",
            "state": {
                "11961b27-2c93-4331-9f4c-a53e994a9907": {
                    "type": "Synapse.DataFrame",
                    "sync_state": {
                        "table": {
                            "rows": [{
                                    "0": "25.36957482259526",
                                    "1": "5.036821897049295",
                                    "2": "0.7641619871170653",
                                    "3": "3.239484827129292",
                                    "index": 1
                                }
                            ],
                            "schema": [{
                                    "key": "0",
                                    "name": "mean_squared_error",
                                    "type": "double"
                                }, {
                                    "key": "1",
                                    "name": "root_mean_squared_error",
                                    "type": "double"
                                }, {
                                    "key": "2",
                                    "name": "R^2",
                                    "type": "double"
                                }, {
                                    "key": "3",
                                    "name": "mean_absolute_error",
                                    "type": "double"
                                }
                            ],
                            "truncated": false
                        },
                        "isSummary": false,
                        "language": "scala"
                    },
                    "persist_state": {
                        "view": {
                            "type": "details",
                            "tableOptions": {},
                            "chartOptions": {
                                "chartType": "bar",
                                "categoryFieldKeys": ["0"],
                                "seriesFieldKeys": ["0"],
                                "aggregationType": "sum",
                                "isStacked": false,
                                "binsNumber": 10
                            }
                        },
                        "customOptions": {}
                    }
                },
                "838a4109-46a5-43e7-92c9-e60eb76d273f": {
                    "type": "Synapse.DataFrame",
                    "sync_state": {
                        "table": {
                            "rows": [{
                                    "0": "25.55949164306082",
                                    "1": "5.055639587931562",
                                    "2": "0.7623965020482438",
                                    "3": "3.2555498844257347",
                                    "index": 1
                                }
                            ],
                            "schema": [{
                                    "key": "0",
                                    "name": "mean_squared_error",
                                    "type": "double"
                                }, {
                                    "key": "1",
                                    "name": "root_mean_squared_error",
                                    "type": "double"
                                }, {
                                    "key": "2",
                                    "name": "R^2",
                                    "type": "double"
                                }, {
                                    "key": "3",
                                    "name": "mean_absolute_error",
                                    "type": "double"
                                }
                            ],
                            "truncated": false
                        },
                        "isSummary": false,
                        "language": "scala"
                    },
                    "persist_state": {
                        "view": {
                            "type": "details",
                            "tableOptions": {},
                            "chartOptions": {
                                "chartType": "bar",
                                "categoryFieldKeys": ["0"],
                                "seriesFieldKeys": ["0"],
                                "aggregationType": "sum",
                                "isStacked": false,
                                "binsNumber": 10
                            }
                        },
                        "customOptions": {}
                    }
                }
            }
        },
        "trident": {
            "lakehouse": {
                "known_lakehouses": [{
                        "id": "4fd9d516-62ef-4c40-9edf-b64bc782ca80"
                    }
                ]
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
