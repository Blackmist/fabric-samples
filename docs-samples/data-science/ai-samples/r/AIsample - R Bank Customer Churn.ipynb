{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9877558c-eb5e-4f13-85ac-1401db3d8e20",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Create, evaluate, and score a churn prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34082526-8276-460a-b625-733b3246a119",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you'll see a Microsoft Fabric data science workflow with an end-to-end example. The scenario is to build a model to predict whether bank customers would churn or not. The churn rate, also known as the rate of attrition refers to the rate at which bank customers stop doing business with the bank.\n",
    "\n",
    "The summary of main steps you take in this notebook are as following:\n",
    "\n",
    "1. Install custom libraries\n",
    "2. Load the data\n",
    "1. Understand and process the data through exploratory data analysis and demonstrate the use of Fabric Data Wrangler feature\n",
    "1. Train machine learning models using Scikit-Learn and LightGBM\n",
    "1. Evaluate and save the final machine learning model\n",
    "1. Demonstrate the model performance via visualizations in Power BI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d292182-25ef-4d19-91ef-480a13d73ae2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prerequisites\n",
    "- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will be downloading data from a public blob, then storing the data in the lakehouse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a441e-30b4-411a-82ed-121019b0e7f8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Install Custom Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77304eb-9992-439d-9142-16b2eab60b67",
   "metadata": {
    "editable": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "source": [
    "When developing a machine learning model or doing ad-hoc data analysis, you may need to quickly install custom libraries (e.g., `imbalance` and `randomForest` in this notebook) for the Apache Spark session. To do this, you have two choices. \n",
    "\n",
    "1. You can use the in-line installation capabilities (e.g., `install.packages`, `devtools::install_version`, etc.) to quickly get started with new libraries. Note that this approach only installs the custom libraries in current notebook and not in the workspace.\n",
    "\n",
    "```R\n",
    "# Use R base to install libraries\n",
    "install.packages(<library name>)\n",
    "\n",
    "# Use devtools to install libraries\n",
    "devtools::install_version(<library name>, version = <library version>)\n",
    "```\n",
    "<br>\n",
    "\n",
    "2. Alternatively, you can install the required libraries in the **workspace**. To do so, navigate to the workspace setting as shown below and then select **Library Management**, switch to the **Custom libraries** tab, select **Upload** to upload the `tar.gz` packages from the local file system, and then select on **Apply**. This will automatically install all selected libraries in the **workspace**.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/R-AI-Samples/Bank Customer Churn/Bank Customer Churn.png\"  width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff5c3e-f3e7-439b-a87a-a5fea538679b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In this notebook, you will be using the `imbalance` and `randomForest` libraries.  To install in the workspace, upload the libraries' dependencies and then the libraries to the workspace. Once successfully uploaded, you can use the the libraries `imbalance` and `randomForest` in the notebook via `library(imbalance)` and `library(randomForest)`, respectively.\n",
    "\n",
    "In the following, you'll use `install.packages()` to install the these libraries.  Set `quiet` to `TRUE` to make output more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9e80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "60895e76-f375-4115-a141-d3fbb1ef49f3",
       "queued_time": "2023-09-12T16:01:33.4080113Z",
       "session_id": null,
       "session_start_time": "2023-09-12T16:01:34.4421001Z",
       "spark_jobs": null,
       "spark_pool": null,
       "state": "session_starting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , SessionStarting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install imbalance for SMOTE\n",
    "install.packages(\"imbalance\", quiet = TRUE)\n",
    "# Install Random Forest algorithm\n",
    "install.packages(\"randomForest\", quiet=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507119fd-2330-45e0-a982-c56bb8b37620",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cf81a-e337-4804-ac9f-9c3fa4e8612d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Dataset\n",
    "\n",
    "The dataset contains churn status of 10000 customers along with 14 attributes that include credit score, geographical location (Germany, France, Spain), gender (male, female), age, tenure (years of being bank's customer), account balance, estimated salary, number of products that a customer has purchased through the bank, credit card status (whether a customer has a credit card or not), and active member status (whether an active bank's customer or not).\n",
    "\n",
    "The dataset also includes columns such as row number, customer ID, and customer surname that should have no impact on customer's decision to leave the bank. The event that defines the customer's churn is the closing of the customer's bank account, therefore, the column `exit` in the dataset refers to customer's abandonment. Since you don't have much context about these attributes, you'll proceed without having background information about the dataset. Your aim is to understand how these attributes contribute to the `exit` status.\n",
    "\n",
    "Out of the 10000 customers, only 2037 customers (around 20%) have left the bank. Therefore, given the class imbalance ratio, it is recommended to generate synthetic data.\n",
    "\n",
    "- churn.csv\n",
    "\n",
    "|\"CustomerID\"|\"Surname\"|\"CreditScore\"|\"Geography\"|\"Gender\"|\"Age\"|\"Tenure\"|\"Balance\"|\"NumOfProducts\"|\"HasCrCard\"|\"IsActiveMember\"|\"EstimatedSalary\"|\"Exited\"|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|15634602|Hargrave|619|France|Female|42|2|0.00|1|1|1|101348.88|1|\n",
    "|15647311|Hill|608|Spain|Female|41|1|83807.86|1|0|1|112542.58|0|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a18afb-6701-4d49-9c36-4bff92a465ad",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Introduction to SMOTE\n",
    "\n",
    "The problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. Synthetic Minority Oversampling Technique (SMOTE) is the most widely used approach to synthesize new samples for the minority class. You can read more about SMOTE [here](https://cran.r-project.org/web/packages/imbalance/imbalance.pdf) and [here](https://cran.r-project.org/web/packages/imbalance/vignettes/imbalance.pdf).\n",
    "\n",
    "You will be able to access SMOTE using the `imbalance` library that you installed in Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734ae43-5a9c-4577-9003-6c454c943dba",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Download dataset and upload to lakehouse\n",
    "\n",
    "This code downloads a publicly available version of the dataset and then stores it in a Fabric lakehouse.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> **Make sure you [add a lakehouse](https://aka.ms/fabric/addlakehouse) to the notebook before running it. Failure to do so will result in an error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2f371-2e4b-4f66-97a8-f003a91f264d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "1c39b66f-d094-4043-8510-b6bcccb08a45",
       "queued_time": "2023-09-12T16:01:33.4096298Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(fs)\n",
    "library(httr)\n",
    "\n",
    "remote_url <- \"https://sdkstorerta.blob.core.windows.net/churnblob\"\n",
    "file_list <- c(\"churn.csv\")\n",
    "download_path <- \"/lakehouse/default/Files/churn/raw\"\n",
    "\n",
    "if (!dir_exists(\"/lakehouse/default\")) {\n",
    "  stop(\"Default lakehouse not found, please add a lakehouse and restart the session.\")\n",
    "}\n",
    "dir_create(download_path, recurse= TRUE)\n",
    "for (fname in file_list) {\n",
    "  if (!file_exists(paste0(download_path, \"/\", fname))) {\n",
    "    r <- GET(paste0(remote_url, \"/\", fname), timeout(30))\n",
    "    writeBin(content(r, \"raw\"), paste0(download_path, \"/\", fname))\n",
    "  }\n",
    "}\n",
    "print(\"Downloaded demo data files into lakehouse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f2aab-6349-404d-be3e-875e6dfb4bc9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Start recording the time it takes to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b463bf-29de-43a5-92dc-43d1098f765b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "37bd49b0-1a74-4c07-a35f-4efc065ad074",
       "queued_time": "2023-09-12T16:01:33.4120991Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record the notebook running time\n",
    "ts <- as.numeric(Sys.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa20da-ca5e-4585-9f22-2ca95a49c21f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Read raw data from the lakehouse\n",
    "\n",
    "Reads raw data from the **Files** section of the lakehouse, adds additional columns for different date parts and the same information will be used to create partitioned delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909a461-ee4d-4e85-8b51-4db3e63918ec",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "efa417f5-c980-4547-9b5d-4a54db9fa38c",
       "queued_time": "2023-09-12T16:01:33.412569Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname <- \"churn.csv\"\n",
    "download_path <- \"/lakehouse/default/Files/churn/raw\"\n",
    "rdf <- readr::read_csv(paste0(download_path, \"/\", fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f0e13-4d11-4e81-a657-0021bf08526e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27397143-89e7-40ab-8a72-5e603449f3e9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Display raw data\n",
    "\n",
    "Perform a preliminary exploration of the raw data using the `head()` or `str()` commands.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e591d69-be93-4fac-873b-9063ed8d6f4d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "27641781-7449-48c5-9d80-4efa0d8ce7aa",
       "queued_time": "2023-09-12T16:01:33.4134146Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(rdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d61baa-5d51-4df2-93ac-9f34c64e038f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Initial data cleansing\n",
    "\n",
    "You first need to convert the R DataFrame to spark DataFrame, then perform the following operations to cleanse the raw dataset.\n",
    "- Drop the rows with missing data across all columns\n",
    "- Drop the duplicate rows across the columns `RowNumber` and `CustomerId`\n",
    "- Drop the columns `RowNumber`, `CustomerId`, and `Surname`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3311f8c-8f4b-47a3-b8a4-568106015506",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "dd5c543f-f6b1-4c8a-aa70-96cda8ffd868",
       "queued_time": "2023-09-12T16:01:33.4147516Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform R DataFrame to spark DataFrame\n",
    "df <- as.DataFrame(rdf)\n",
    "\n",
    "clean_data <- function(df) {\n",
    "  sdf <- df %>%\n",
    "    # Drop rows with missing data across all columns\n",
    "    na.omit() %>%\n",
    "    # Drop duplicate rows in columns: 'RowNumber', 'CustomerId'\n",
    "    dropDuplicates(c(\"RowNumber\", \"CustomerId\")) %>%\n",
    "    # Drop columns: 'RowNumber', 'CustomerId', 'Surname'\n",
    "    SparkR::select(\"CreditScore\", \"Geography\", \"Gender\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"Exited\")\n",
    "  return(sdf)\n",
    "}\n",
    "\n",
    "df_clean <- clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614623de-0753-444e-b0d5-e70b06fff703",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You can now explore the spark Dataframe with `display` command, do some basic statistics, and even show chart views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c510ec-b8d1-4756-bf23-ee844f304904",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "db93c60b-0118-40cb-8919-6fbe2dc4fad5",
       "queued_time": "2023-09-12T16:01:33.415403Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb37710-4b16-4483-b516-92a085280daa",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Determine categorical, numerical, and target attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a552a5b-2504-489e-a162-72cd0d3d6dab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "49174dcf-c35e-4985-9fee-5c28a6d56d92",
       "queued_time": "2023-09-12T16:01:33.4166524Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the dependent (target) attribute\n",
    "dependent_variable_name <- \"Exited\"\n",
    "print(dependent_variable_name)\n",
    "\n",
    "# Get the distinct values for each column\n",
    "exprs = lapply(names(df_clean), function(x) alias(countDistinct(df_clean[[x]]), x))\n",
    "# Use do.call to splice the aggregation expressions to aggregate function\n",
    "distinct_value_number <- SparkR::collect(do.call(agg, c(x = df_clean, exprs)))\n",
    "\n",
    "# Determine the categorical attributes\n",
    "categorical_variables <- names(df_clean)[sapply(names(df_clean), function(col) col %in% c(\"0\") || distinct_value_number[[col]] <= 5 && !(col %in% c(dependent_variable_name)))]\n",
    "print(categorical_variables)\n",
    "\n",
    "# Determine the numerical attributes\n",
    "numeric_variables <- names(df_clean)[sapply(names(df_clean), function(col) coltypes(SparkR::select(df_clean, col)) == \"numeric\" && distinct_value_number[[col]] > 5)]\n",
    "print(numeric_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc759d53-bb16-4887-915a-acd88218e188",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Convert the spark DataFrame to R DataFrame for easier processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ea117-97c8-4f67-a4f3-4a34151ce8ad",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "3609c983-85ab-4dbc-a491-b2167305a023",
       "queued_time": "2023-09-12T16:01:33.4171921Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transfrom spark DataFrame to R DataFrame\n",
    "rdf_clean <- SparkR::collect(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fceb0-e058-453e-b57d-d189682e95f6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### The five-number summary \n",
    "\n",
    "Show the five-number summary (the minimum score, first quartile, median, third quartile, the maximum score) for the numerical attributes, using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8d329-ca68-461f-b660-7bff53cc79be",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "4a7c2440-ca8c-4453-a10a-e1626cbc6ebc",
       "queued_time": "2023-09-12T16:01:33.4195146Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the overall layout of the graphics window\n",
    "par(mfrow = c(2, 1), \n",
    "    mar = c(2, 1, 2, 1)) # margin size\n",
    "\n",
    "for(item in numeric_variables[1:2]){\n",
    "    # Create boxplot\n",
    "    boxplot(rdf_clean[, item], \n",
    "            main = item, \n",
    "            col = \"darkgreen\", \n",
    "            cex.main = 1.5, # title size\n",
    "            cex.lab = 1.3, # axis label size\n",
    "            cex.axis = 1.2,\n",
    "            horizontal = TRUE) # axis size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e7e64-f466-4947-b0e2-8c9a9bcd5c2b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "4081ecab-61b6-40e4-a97f-43966d660817",
       "queued_time": "2023-09-12T16:01:33.4202749Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the overall layout of the graphics window\n",
    "par(mfrow = c(3, 1), \n",
    "    mar = c(2, 1, 2, 1)) # margin size\n",
    "\n",
    "for(item in numeric_variables[3:5]){\n",
    "    # Create boxplot\n",
    "    boxplot(rdf_clean[, item], \n",
    "            main = item, \n",
    "            col = \"darkgreen\", \n",
    "            cex.main = 1.5, # title size\n",
    "            cex.lab = 1.3, # axis label size\n",
    "            cex.axis = 1.2,\n",
    "            horizontal = TRUE) # axis size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212651d-e654-40bf-9d82-4b3ddca01f91",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Distribution of exited and non-exited customers \n",
    "\n",
    "Show the distribution of exited versus non-exited customers across the categorical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca985c-f818-4340-8ce4-397f8b0eee9e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "ef11b41a-70bf-46da-8e95-ce55bff41a40",
       "queued_time": "2023-09-12T16:01:33.4210187Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attr_list <- c('Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Tenure')\n",
    "par(mfrow = c(2, 1), \n",
    "    mar = c(2, 1, 2, 1)) # margin size\n",
    "for (item in attr_list[1:2]) {\n",
    "    counts <- table(rdf_clean$Exited, rdf_clean[,item])\n",
    "    barplot(counts, main=item, col=c(\"darkblue\",\"yellow\"), \n",
    "            cex.main = 1.5, # title size\n",
    "            cex.axis = 1.2,\n",
    "            legend = rownames(counts), beside=TRUE)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4c776-b486-4eb6-bd9d-57b9720457bc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "d6a48a84-ad57-44e6-a535-7d597072b2fb",
       "queued_time": "2023-09-12T16:01:33.4217608Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mfrow = c(2, 1), \n",
    "    mar = c(2, 2, 2, 1)) # margin size\n",
    "for (item in attr_list[3:4]) {\n",
    "    counts <- table(rdf_clean$Exited, rdf_clean[,item])\n",
    "    barplot(counts, main=item, col=c(\"darkblue\",\"yellow\"), \n",
    "            cex.main = 1.5, # title size\n",
    "            cex.axis = 1.2,\n",
    "            legend = rownames(counts), beside=TRUE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c49826-27a0-4eaf-bc8f-92b3d113fb06",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "5c966425-6c7c-4919-bca2-2376d8704981",
       "queued_time": "2023-09-12T16:01:33.4235624Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mfrow = c(2, 1), \n",
    "    mar = c(2, 1, 2, 1)) # margin size\n",
    "for (item in attr_list[5:6]) {\n",
    "    counts <- table(rdf_clean$Exited, rdf_clean[,item])\n",
    "    barplot(counts, main=item, col=c(\"darkblue\",\"yellow\"), \n",
    "            cex.main = 1.5, # title size\n",
    "            cex.axis = 1.2,\n",
    "            legend = rownames(counts), beside=TRUE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471cf4ca-e525-4efd-8bea-63ae5b5c819e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Distribution of numerical attributes\n",
    "\n",
    "Show the the frequency distribution of numerical attributes using histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3638293-f41b-4d99-b732-77825f4fc73d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "bac3a045-9978-4414-84ea-587106454d87",
       "queued_time": "2023-09-12T16:01:33.4243072Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the overall layout of the graphics window\n",
    "par(mfrow = c(2, 1), \n",
    "    mar = c(2, 4, 2, 4) + 0.1) # margin size\n",
    "\n",
    "# Create histograms\n",
    "for (item in numeric_variables[1:2]) {\n",
    "    hist(rdf_clean[, item], \n",
    "         main = item, \n",
    "         col = \"darkgreen\", \n",
    "         xlab = item,\n",
    "         cex.main = 1.5, # title size\n",
    "         cex.axis = 1.2,\n",
    "         breaks = 20) # number of bins\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e978b2-e1f4-4da6-8f22-80b3a060838d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "523d0d16-ed0a-42b5-a68d-94cb0b7a378b",
       "queued_time": "2023-09-12T16:01:33.4249671Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the overall layout of the graphics window\n",
    "par(mfrow = c(3, 1), \n",
    "    mar = c(2, 4, 2, 4) + 0.1) # margin size\n",
    "\n",
    "# Create histograms\n",
    "for (item in numeric_variables[3:5]) {\n",
    "    hist(rdf_clean[, item], \n",
    "         main = item, \n",
    "         col = \"darkgreen\", \n",
    "         xlab = item,\n",
    "         cex.main = 1.5, # title size\n",
    "         cex.axis = 1.2,\n",
    "         breaks = 20) # number of bins\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd326da-9e2c-4580-9b0d-d747fb142e04",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Perform feature engineering\n",
    "\n",
    "The following feature engineering generates new attributes based on current attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e1405-6a9f-49da-b1dc-a1d3e356acd5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "aed3702d-9bd2-403e-9e6e-682a81f6df18",
       "queued_time": "2023-09-12T16:01:33.4260619Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdf_clean$NewTenure <- rdf_clean$Tenure / rdf_clean$Age\n",
    "rdf_clean$NewCreditsScore <- as.numeric(cut(rdf_clean$CreditScore, breaks=quantile(rdf_clean$CreditScore, probs=seq(0, 1, by=1/6)), include.lowest=TRUE, labels=c(1, 2, 3, 4, 5, 6)))\n",
    "rdf_clean$NewAgeScore <- as.numeric(cut(rdf_clean$Age, breaks=quantile(rdf_clean$Age, probs=seq(0, 1, by=1/8)), include.lowest=TRUE, labels=c(1, 2, 3, 4, 5, 6, 7, 8)))\n",
    "rdf_clean$NewBalanceScore <- as.numeric(cut(rank(rdf_clean$Balance), breaks=quantile(rank(rdf_clean$Balance, ties.method = \"first\"), probs=seq(0, 1, by=1/5)), include.lowest=TRUE, labels=c(1, 2, 3, 4, 5)))\n",
    "rdf_clean$NewEstSalaryScore <- as.numeric(cut(rdf_clean$EstimatedSalary, breaks=quantile(rdf_clean$EstimatedSalary, probs=seq(0, 1, by=1/10)), include.lowest=TRUE, labels=c(1:10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d99e2-ddde-4f08-b977-024c450821f1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Perform one-hot encoding\n",
    "\n",
    "Use one-hot encoding to convert the categorical attributes to the numerical ones so that they can be fed into the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04c0d6-4a0d-49d3-9803-ba83b55aee48",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "7099f9cd-f929-4a60-b8f2-0e48a6482bb3",
       "queued_time": "2023-09-12T16:01:33.426781Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdf_clean <- cbind(rdf_clean, model.matrix(~Geography+Gender-1, data=rdf_clean))\n",
    "rdf_clean <- subset(rdf_clean, select = - c(Geography, Gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6668cfe1-0ed0-4e76-97fd-56ad3c6b189f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create a delta table to generate the Power BI report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5871860-8a6a-4c30-b6ef-f289479bb9f7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "eb7c474c-4c6d-4ac5-801d-65d1520ea08e",
       "queued_time": "2023-09-12T16:01:33.427518Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name <- \"rdf_clean\"\n",
    "# Create Spark DataFrame from R data frame\n",
    "sparkDF <- as.DataFrame(rdf_clean)\n",
    "write.df(sparkDF, paste0(\"Tables/\", table_name), source = \"delta\", mode = \"overwrite\")\n",
    "cat(paste0(\"Spark dataframe saved to delta table: \", table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ded25-18e9-46de-800e-8d1c2cfbc358",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Summary of observations from the exploratory data analysis\n",
    "\n",
    "- Most of the customers are from France comparing to Spain and Germany, while Spain has the lowest churn rate comparing to France and Germany.\n",
    "- Most of the customers have credit cards.\n",
    "- There are customers whose age and credit score are above 60 and below 400, respectively, but they can't be considered as outliers.\n",
    "- Very few customers have more than two of the bank's products.\n",
    "- Customers who are not active have a higher churn rate.\n",
    "- Gender and tenure years don't seem to have an impact on customer's decision to close the bank account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca639b4-64e4-40dd-b779-e859587e93c8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Model rraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a74bc2-7d45-4513-acba-4224ba0f5003",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "With your data in place, you can now define the model. You'll apply Random Forrest and LightGBM models in this notebook. You will leverage `randomForest` and `lightgbm` to implement the models within a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba69fe-3ef8-480f-a44d-5a30c088e7dc",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load the delta table from the lakehouse\n",
    "\n",
    "You may use other delta tables considering the lakehouse as the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d9f2f-612e-4011-9d61-a0fc8edc2e47",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "8cd53c49-c262-42d9-8fb1-5a8e94758198",
       "queued_time": "2023-09-12T16:01:33.4281893Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SEED <- 12345\n",
    "rdf_clean <- read.df(\"Tables/rdf_clean\", source = \"delta\")\n",
    "df_clean <- as.data.frame(rdf_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac51d3a-6d8e-487a-9091-3eff57d3a3cc",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Import randomForest and lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfad39e-8d1a-4bd2-8e82-ae40dab1670b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "ad0004cc-a29f-420c-b719-32f3b9ee2134",
       "queued_time": "2023-09-12T16:01:37.4648476Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(randomForest)\n",
    "library(lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65bde63-541e-4268-8a1e-57b3b172e10a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Prepare training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7e089-016a-43c5-8c39-36855d38ab4b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "df55a823-bd47-4bfc-97be-de56c799fa3b",
       "queued_time": "2023-09-12T16:01:38.6817687Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(SEED)\n",
    "y <- factor(df_clean$Exited)\n",
    "X <- df_clean[, !(colnames(df_clean) %in% c(\"Exited\"))]\n",
    "split <- base::sample(c(TRUE, FALSE), nrow(df_clean), replace = TRUE, prob = c(0.8, 0.2))\n",
    "X_train <- X[split,]\n",
    "X_test <- X[!split,]\n",
    "y_train <- y[split]\n",
    "y_test <- y[!split]\n",
    "train_df <- cbind(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5257f-668f-48bf-b932-b20c9dbacf05",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Apply SMOTE to the training data to synthesize new samples for the minority class\n",
    "\n",
    "SMOTE should only be applied to the training dataset. You must leave the test dataset in its original imbalanced distribution in order to get a valid approximation of how the model will perform on the original data, which is representing the situation in production.\n",
    "\n",
    "Start by showing the distribution of classes in the dataset in order to find out which class is the minority class. The ratio of minority class to majority class is defined as `imbalace ratio` in imbalance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae69146-27b8-4475-b4d8-5ab77c9892ca",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "89636dc8-17cf-4a76-b587-e3d1704ac74c",
       "queued_time": "2023-09-12T16:01:38.941686Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_ratio <- imbalance::imbalanceRatio(train_df, classAttr = \"y_train\")\n",
    "message(sprintf(\"Original imbalance ratio is %.2f%% as {Size of minority class}/{Size of majority class}.\", original_ratio * 100))\n",
    "message(sprintf(\"Positive class(Exited) takes %.2f%% of the dataset.\", round(sum(train_df$y_train == 1)/nrow(train_df) * 100, 2)))\n",
    "message(sprintf(\"Negatvie class(Non-Exited) takes %.2f%% of the dataset.\", round(sum(train_df$y_train == 0)/nrow(train_df) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee02b5-f050-4884-8bc8-5bae4bced646",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In the training dataset, the `positive class (Exited)` refers the minority class which takes 20.34% of the dataset and `negative class (Non-Exited)` refers to the majority class that takes 79.66% of the dataset. \n",
    "\n",
    "The next cell rewrites the oversample function in `imbalance` library in order to generate a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85771a69-9a16-4cef-b85e-b17a7b335395",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "640de3ac-4235-48d9-b2c7-e85fd455896c",
       "queued_time": "2023-09-12T16:01:39.1731679Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binary_oversample <- function(train_df, X_train, y_train, class_Attr = \"Class\"){\n",
    "    negative_num <- sum(y_train == 0) # Compute the number of negative class\n",
    "    positive_num <- sum(y_train == 1) # Compute the number of positive class\n",
    "    difference_num <- abs(negative_num - positive_num) # Compute the difference between negative and positive class\n",
    "    originalShape <- imbalance:::datasetStructure(train_df, class_Attr) # Get the original dataset schema\n",
    "    new_samples <- smotefamily::SMOTE(X_train, y_train, dup_size = ceiling(max(negative_num, positive_num)/min(negative_num, positive_num))) # Use SMOTE to oversample\n",
    "    new_samples <- new_samples$syn_data # Get the synthetic data\n",
    "    new_samples <- new_samples[base::sample(1:nrow(new_samples), size = difference_num), ] # Sample and shuffle the synthetic data\n",
    "    new_samples <- new_samples[, -ncol(new_samples)] # Remove the class colomn\n",
    "    new_samples <- imbalance:::normalizeNewSamples(originalShape, new_samples) # Normalize the synthetic data\n",
    "    new_train_df <- rbind(train_df, new_samples) # Concat original and synthetic data by row\n",
    "    new_train_df <- new_train_df[base::sample(nrow(new_train_df)), ] # shuffle the training dataset\n",
    "    new_train_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67119b-af70-4369-9cc3-8ecb3e6d44e6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Oversample the training dataset \n",
    "\n",
    "Use the newly defined oversample function to perform oversampling on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed7350-1dcb-4487-9b08-1248f09a0ff9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "726d9fcf-4506-414d-8e80-4d64f523628f",
       "queued_time": "2023-09-12T16:01:39.3754831Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(dplyr)\n",
    "new_train_df <- binary_oversample(train_df, X_train, y_train, class_Attr=\"y_train\")\n",
    "smote_ratio <- imbalance::imbalanceRatio(new_train_df, classAttr = \"y_train\")\n",
    "message(sprintf(\"Imbalance ratio after using smote is %.2f%%\\n\", smote_ratio * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf44ba6-afea-4401-ac2a-157794d02a2b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c4ec8-75c6-42fe-a2e1-24fdba41844f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Train the model using Random Forest with four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560e97e-f4ba-4c73-abe3-abf8893afd30",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "abde95e2-e6e3-48c5-921f-a6cd7a36a49a",
       "queued_time": "2023-09-12T16:01:39.5871877Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "rfc1_sm <- randomForest(y_train ~ ., data = new_train_df, ntree = 500, mtry = 4, nodesize = 3)\n",
    "y_pred <- predict(rfc1_sm, X_test, type = \"response\")\n",
    "cr_rfc1_sm <- caret::confusionMatrix(y_pred, y_test)\n",
    "cm_rfc1_sm <- table(y_pred, y_test)\n",
    "roc_auc_rfc1_sm <- pROC::auc(pROC::roc(as.numeric(y_test), as.numeric(y_pred)))\n",
    "print(paste0(\"The auc is \", roc_auc_rfc1_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4aebc8-c4f3-4a62-98e1-85653d19b78f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Train the model using Random Forest with six features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370dc3a-e35b-4206-89cc-e56f4a4b10b4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "5c78f2ea-e834-40b7-8e17-f8f1ed6d8227",
       "queued_time": "2023-09-12T16:01:39.8380169Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfc2_sm <- randomForest(y_train ~ ., data = new_train_df, ntree = 500, mtry = 6, nodesize = 3)\n",
    "y_pred <- predict(rfc2_sm, X_test, type = \"response\")\n",
    "cr_rfc2_sm <- caret::confusionMatrix(y_pred, y_test)\n",
    "cm_rfc2_sm <- table(y_pred, y_test)\n",
    "roc_auc_rfc2_sm <- pROC::auc(pROC::roc(as.numeric(y_test), as.numeric(y_pred)))\n",
    "print(paste0(\"The auc is \", roc_auc_rfc2_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c83ff0-38ce-4ef9-995b-6f2bee2716a6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Train the model using LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48726506-0451-4a19-a187-f503c7b8323b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "2361dd21-3a51-4726-bf0d-adf8145fbc70",
       "queued_time": "2023-09-12T16:01:40.0576625Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(42)\n",
    "X_train <- new_train_df[, !(colnames(new_train_df) %in% c(\"y_train\"))]\n",
    "y_train <- as.numeric(as.character(new_train_df$y_train))\n",
    "y_test <- as.numeric(as.character(y_test))\n",
    "lgbm_sm_model <- lgb.train(list(objective = \"binary\", learning_rate = 0.1, max_delta_step = 2, nrounds = 100, max_depth = 10, eval_metric = \"logloss\"), lgb.Dataset(as.matrix(X_train), label = as.vector(y_train)), valids = list(test = lgb.Dataset(as.matrix(X_test), label = as.vector(as.numeric(y_test)))))\n",
    "y_pred <- as.numeric(predict(lgbm_sm_model, as.matrix(X_test)) > 0.5)\n",
    "accuracy <- mean(y_pred == as.vector(y_test))\n",
    "cr_lgbm_sm <- caret::confusionMatrix(as.factor(y_pred), as.factor(as.vector(y_test)))\n",
    "cm_lgbm_sm <- table(y_pred, as.vector(y_test))\n",
    "roc_auc_lgbm_sm <- pROC::auc(pROC::roc(as.vector(y_test), y_pred))\n",
    "print(paste0(\"The auc is \", roc_auc_lgbm_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0dc58-0613-448d-b52e-5932fb2d69e2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Evaluate and save the final machine learning model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb574b-5c21-436c-9fb2-be8144e853c6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Assess the performances of the saved models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed281a-bd46-49ac-b3e2-f97895a0c350",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "910bb1d4-96cc-4608-8ada-8886ac22c101",
       "queued_time": "2023-09-12T16:01:40.2611969Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ypred_rfc1_sm <- predict(rfc1_sm, X_test, type = \"response\")\n",
    "ypred_rfc2_sm <- predict(rfc2_sm, X_test, type = \"response\")\n",
    "ypred_lgbm1_sm <- as.numeric(predict(lgbm_sm_model, as.matrix(X_test)) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae531781-ac6a-4984-b91a-1ed756a48ba3",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    " #### Show True/False Positives/Negatives using the Confusion Matrix\n",
    " \n",
    " Develop a script to plot the confusion matrix in order to evaluate the accuracy of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77abff-6a42-4dfe-bb26-1d4d4f6a8dcb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "85478cbf-19f1-48a3-a9ad-699aa7fe42e9",
       "queued_time": "2023-09-12T16:01:40.4964833Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix <- function(cm, classes, normalize=FALSE, title='Confusion matrix', cmap=heat.colors(10)) {\n",
    "  if (normalize) {\n",
    "    cm <- cm / rowSums(cm)\n",
    "  }\n",
    "  op <- par(mar = c(6,6,3,1))\n",
    "  image(1:nrow(cm), 1:ncol(cm), t(cm[nrow(cm):1,]), col = cmap, xaxt = 'n', yaxt = 'n', main = title, xlab = \"Prediction\", ylab = \"Reference\")\n",
    "  axis(1, at = 1:nrow(cm), labels = classes, las = 2)\n",
    "  axis(2, at = 1:ncol(cm), labels = rev(classes))\n",
    "  for (i in seq_len(nrow(cm))) {\n",
    "    for (j in seq_len(ncol(cm))) {\n",
    "      text(i, ncol(cm) - j + 1, cm[j,i], cex = 0.8)\n",
    "    }\n",
    "  }\n",
    "  par(op)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d0968-e975-494a-97a8-c4ef460efafc",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Create the confusion matrix for Random Forest Classifier with four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd9303-f174-44ee-9bb2-cbbfdf60e48d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "315d7156-369e-4998-85c8-79c22d102cd0",
       "queued_time": "2023-09-12T16:01:40.7039896Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm <- table(y_test, ypred_rfc1_sm)\n",
    "plot_confusion_matrix(cfm, classes=c('Non Churn','Churn'), title='Random Forest with features of 4')\n",
    "tn <- cfm[1,1]\n",
    "fp <- cfm[1,2]\n",
    "fn <- cfm[2,1]\n",
    "tp <- cfm[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f6e78-a3ad-4c39-a6a6-70c358fd6c0e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Create the confusion matrix for Random Forest Classifier with six features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d980871-24b2-4dd2-941e-18ef72fc4464",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "313353d9-f2f1-4e84-b4b7-9d476e2c6523",
       "queued_time": "2023-09-12T16:01:40.9210436Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm <- table(y_test, ypred_rfc2_sm)\n",
    "plot_confusion_matrix(cfm, classes=c('Non  Churn','Churn'), title='Random Forest with features of 6')\n",
    "tn <- cfm[1,1]\n",
    "fp <- cfm[1,2]\n",
    "fn <- cfm[2,1]\n",
    "tp <- cfm[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e4d79-885e-47fb-9962-1c8366a5ab5c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Create the confusion matrix for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd3fb5-1dba-49f8-b24a-526d91c82607",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "5c714d00-7957-450b-867e-df03a10477c2",
       "queued_time": "2023-09-12T16:01:41.1217079Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm <- table(y_test, ypred_lgbm1_sm)\n",
    "plot_confusion_matrix(cfm, classes=c('Non Churn','Churn'), title='LightGBM')\n",
    "tn <- cfm[1,1]\n",
    "fp <- cfm[1,2]\n",
    "fn <- cfm[2,1]\n",
    "tp <- cfm[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823395a-c7b2-4e31-ab0b-5af00fc7ca23",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Save results for Power BI\n",
    "\n",
    "Move model prediction results to Power BI Visualization by saving delta frame to lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed04b0-3239-4d08-a07c-6b82513ce0ff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "72319866-f069-4dfc-ac2c-04d67da45c5f",
       "queued_time": "2023-09-12T16:01:41.3368419Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pred <- X_test\n",
    "df_pred$y_test <- y_test\n",
    "df_pred$ypred_rfc1_sm <- ypred_rfc1_sm\n",
    "df_pred$ypred_rfc2_sm <- ypred_rfc2_sm\n",
    "df_pred$ypred_lgbm1_sm <- ypred_lgbm1_sm\n",
    "\n",
    "table_name <- \"df_pred_results\"\n",
    "sparkDF <- as.DataFrame(df_pred)\n",
    "write.df(sparkDF, paste0(\"Tables/\", table_name), source = \"delta\", mode = \"overwrite\", overwriteSchema = \"true\")\n",
    "\n",
    "cat(paste0(\"Spark dataframe saved to delta table: \", table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579023b-d67e-49a4-9799-9de8d36b1455",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 6: Business Intelligence via Visualizations in Power BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a929b2-3416-49a6-8603-7f2e9940605d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Use these steps to access your saved table in Power BI.\n",
    "\n",
    "1. On the left, select **OneLake data hub**.\n",
    "1. Select the lakehouse that you added to this notebook.\n",
    "1. On the top right, select **Open** under the section titled **Open this Lakehouse**.\n",
    "1. Select New Power BI dataset on the top ribbon and select `df_pred_results`, then select **Continue** to create a new Power BI dataset linked to the predictions.\n",
    "1. On the tools at the top of the dataset page, select **New report** to open the Power BI report authoring page.\n",
    "\n",
    "Some example visualizations are shown here. The data panel shows the delta tables and columns from the table to select. Upon selecting appropriate x and y axes, you can pick the filters and functions, for example, sum or average of the table column.\n",
    "\n",
    "> [!NOTE]\n",
    "> This shows an illustrated example of how you would analyze the saved prediction results in Power BI. However, for a real customer churn use-case, the platform user may have to do more thorough ideation of what visualizations to create, based on subject matter expertise, and what their firm and business analytics team has standardized as metrics.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn/PBIviz3.png\"  width=\"100%\" height=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05dcad4",
   "metadata": {},
   "source": [
    "The Power BI report shows that customers who use more than two of the bank products have a higher churn rate although few customers had more than two products. The bank should collect more data, but also investigate other features correlated with more products (see the plot in the bottom left panel).\n",
    "Bank customers in Germany have a higher churn rate than in France and Spain (see the plot in the bottom right panel), which suggests that an investigation into what has encouraged customers to leave could be beneficial.\n",
    "There are more middle aged customers (between 25-45) and customers between 45-60 tend to exit more.\n",
    "Finally, customers with lower credit scores would most likely leave the bank for other financial institutes. The bank should look into ways that encourage customers with lower credit scores and account balances to stay with the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2c620-7f1d-4099-8079-5f6744fdb7a1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "livy_statement_state": null,
       "parent_msg_id": "15124cf0-dc8d-49fc-afab-59b0537a933d",
       "queued_time": "2023-09-12T16:01:41.5422886Z",
       "session_id": null,
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": null,
       "state": "waiting",
       "statement_id": null
      },
      "text/plain": [
       "StatementMeta(, , , Waiting, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the entire runtime\n",
    "cat(paste0(\"Full run cost \", as.integer(Sys.time() - ts), \" seconds.\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "r"
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
